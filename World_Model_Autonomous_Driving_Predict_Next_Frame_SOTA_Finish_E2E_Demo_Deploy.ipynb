{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Building Hybrid SOTA World Model**"
      ],
      "metadata": {
        "id": "I9LCNeGMY7gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SEtup & Install ---\n",
        "!pip install -q transformers accelerate peft bitsandbytes\n",
        "!pip install -q matplotlib pillow tqdm decord\n",
        "!pip install -q timm einops"
      ],
      "metadata": {
        "id": "Bb-assmdb8lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Data in from Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oyJLyXkWfPln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GPU Game ---\n",
        "import torch\n",
        "torch.cuda.is_available()\n",
        "gpu_game = !nvidia-simi --query-gpu=gpu_name --format=csv,noheader\n",
        "print(f\"GPU in use: {gpu_name[0]}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Device count: {torch.cuda.device_count()}\")\n",
        "\n",
        "print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "l0ShW78YUR2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# CELL 1: Setup & Install\n",
        "# ============================================\n",
        "!pip install -q transformers accelerate peft bitsandbytes\n",
        "!pip install -q matplotlib pillow tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# ============================================\n",
        "# CELL 2: Model Definition (SIMPLIFIED)\n",
        "# ============================================\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoProcessor\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "\n",
        "class InternVLQwenWorldModel(nn.Module):\n",
        "    \"\"\"\n",
        "    SOTA World Model (SIMPLIFIED):\n",
        "    - InternVL 3.5-8B for perception (using simple processor)\n",
        "    - Qwen3-4B-Thinking for world dynamics\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_lora=True, qwen_size=\"4B\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. PERCEPTION: InternVL 3.5-8B\n",
        "        print(\"Loading InternVL 3.5-8B...\")\n",
        "        self.perception = AutoModel.from_pretrained(\n",
        "            'OpenGVLab/InternVL3_5-8B',\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Use AutoProcessor (SIMPLE!)\n",
        "        self.vl_processor = AutoProcessor.from_pretrained(\n",
        "            'OpenGVLab/InternVL3_5-8B',\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # 2. WORLD MODEL: Qwen3\n",
        "        qwen_models = {\n",
        "            \"4B\": \"Qwen/Qwen3-4B-Thinking\",\n",
        "            \"1.7B\": \"Qwen/Qwen3-1.7B-Instruct\",\n",
        "        }\n",
        "\n",
        "        qwen_path = qwen_models.get(qwen_size, \"Qwen/Qwen3-1.7B-Instruct\")  # Default to 1.7B for speed\n",
        "        print(f\"Loading {qwen_path}...\")\n",
        "\n",
        "        self.world_model = AutoModelForCausalLM.from_pretrained(\n",
        "            qwen_path,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.wm_tokenizer = AutoTokenizer.from_pretrained(qwen_path)\n",
        "\n",
        "        # Hidden sizes\n",
        "        self.internvl_hidden = 4096\n",
        "        self.qwen_hidden = 896\n",
        "\n",
        "        # 3. LoRA (only on Qwen3 for speed)\n",
        "        if use_lora:\n",
        "            print(\"Applying LoRA to Qwen3...\")\n",
        "\n",
        "            # Freeze InternVL (it's already SOTA)\n",
        "            for param in self.perception.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # LoRA on Qwen3 only\n",
        "            lora_config = LoraConfig(\n",
        "                r=32,\n",
        "                lora_alpha=64,\n",
        "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "                lora_dropout=0.1,\n",
        "                bias=\"none\"\n",
        "            )\n",
        "\n",
        "            self.world_model = get_peft_model(self.world_model, lora_config)\n",
        "            print(\"âœ… Qwen3 LoRA applied\")\n",
        "\n",
        "        # 4. Projection\n",
        "        self.projection = nn.Linear(self.qwen_hidden, self.internvl_hidden)\n",
        "\n",
        "        print(\"âœ… Model ready!\")\n",
        "\n",
        "    def perceive(self, image, text):\n",
        "        \"\"\"\n",
        "        Extract features using InternVL (SIMPLE VERSION).\n",
        "        \"\"\"\n",
        "        # Simple processing with AutoProcessor\n",
        "        inputs = self.vl_processor(\n",
        "            text=text,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.perception.device)\n",
        "\n",
        "        # Get features\n",
        "        with torch.no_grad():\n",
        "            outputs = self.perception(**inputs, output_hidden_states=True)\n",
        "            features = outputs.hidden_states[-1][:, -1, :]  # [batch, 4096]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def predict_future(self, scene_features, action):\n",
        "        \"\"\"Use Qwen3 to predict next state.\"\"\"\n",
        "\n",
        "        prompt = f\"Predict next driving state. Action: steer={action[0].item():.2f}, speed={action[1].item():.1f}m/s\"\n",
        "\n",
        "        inputs = self.wm_tokenizer(prompt, return_tensors=\"pt\").to(self.world_model.device)\n",
        "\n",
        "        outputs = self.world_model(**inputs, output_hidden_states=True, return_dict=True)\n",
        "        prediction_features = outputs.hidden_states[-1][:, -1, :]\n",
        "\n",
        "        return prediction_features\n",
        "\n",
        "    def forward(self, current_image, text, action):\n",
        "        \"\"\"Full forward pass.\"\"\"\n",
        "        current_features = self.perceive(current_image, text)\n",
        "        predicted_wm_features = self.predict_future(current_features, action)\n",
        "        predicted_vl_features = self.projection(predicted_wm_features)\n",
        "        return predicted_vl_features\n",
        "\n",
        "    def compute_loss(self, current_image, text, action, future_image):\n",
        "        \"\"\"MSE loss.\"\"\"\n",
        "        predicted = self.forward(current_image, text, action)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            true_next = self.perceive(future_image, text)\n",
        "\n",
        "        loss = nn.functional.mse_loss(predicted, true_next)\n",
        "        return loss\n",
        "\n",
        "print(\"Initializing model...\")\n",
        "model = InternVLQwenWorldModel(use_lora=True, qwen_size=\"1.7B\")  # Use 1.7B for speed\n",
        "print(\"âœ… Model ready!\")\n",
        "\n",
        "# ============================================\n",
        "# CELL 3: FIXED Dataset (Properly Sequential!)\n",
        "# ============================================\n",
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "class SequentialDriveLMDataset(Dataset):\n",
        "    \"\"\"\n",
        "    FIXED: Actually creates temporal sequences!\n",
        "    Groups by scene_token, sorts by timestamp.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_path, image_dir, max_samples=100):\n",
        "        with open(json_path, 'r') as f:\n",
        "            all_data = json.load(f)\n",
        "\n",
        "        print(\"Building temporal sequences...\")\n",
        "\n",
        "        # Group by scene\n",
        "        scenes = defaultdict(list)\n",
        "        for sample in all_data:\n",
        "            scene_token = sample.get('scene_token', 'unknown')\n",
        "            scenes[scene_token].append(sample)\n",
        "\n",
        "        # Sort each scene by timestamp\n",
        "        for scene_token in scenes:\n",
        "            scenes[scene_token].sort(key=lambda x: x.get('timestamp', 0))\n",
        "\n",
        "        # Build sequential pairs\n",
        "        self.samples = []\n",
        "\n",
        "        for scene_token, scene_samples in scenes.items():\n",
        "            # Create pairs: (frame_t, frame_t+1)\n",
        "            for i in range(len(scene_samples) - 1):\n",
        "                current = scene_samples[i]\n",
        "                next_sample = scene_samples[i + 1]\n",
        "\n",
        "                # Check images exist\n",
        "                curr_path = os.path.join(image_dir, current['key_frame'])\n",
        "                next_path = os.path.join(image_dir, next_sample['key_frame'])\n",
        "\n",
        "                if os.path.exists(curr_path) and os.path.exists(next_path):\n",
        "                    # Get text description\n",
        "                    text = \"What is happening in this driving scene?\"\n",
        "                    if 'QA' in current and current['QA']:\n",
        "                        text = current['QA'][0]['q']\n",
        "\n",
        "                    self.samples.append({\n",
        "                        'current': curr_path,\n",
        "                        'next': next_path,\n",
        "                        'text': text,\n",
        "                        'scene': scene_token,\n",
        "                        'time_delta': next_sample.get('timestamp', 0) - current.get('timestamp', 0)\n",
        "                    })\n",
        "\n",
        "                    # Stop if we have enough\n",
        "                    if len(self.samples) >= max_samples:\n",
        "                        break\n",
        "\n",
        "            if len(self.samples) >= max_samples:\n",
        "                break\n",
        "\n",
        "        print(f\"âœ… Created {len(self.samples)} temporal sequences\")\n",
        "        print(f\"   From {len(scenes)} different scenes\")\n",
        "        if self.samples:\n",
        "            avg_delta = sum(s['time_delta'] for s in self.samples) / len(self.samples)\n",
        "            print(f\"   Avg time between frames: {avg_delta/1e6:.2f} seconds\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        return {\n",
        "            'current': Image.open(s['current']).convert('RGB'),\n",
        "            'next': Image.open(s['next']).convert('RGB'),\n",
        "            'text': s['text'],\n",
        "            'action': torch.tensor([[0.0, 5.0]]),  # Default action\n",
        "            'scene': s['scene']\n",
        "        }\n",
        "\n",
        "dataset = SequentialDriveLMDataset(\n",
        "    '/content/drive/MyDrive/drivelm/v1_1_train_nus.json',\n",
        "    '/content/drive/MyDrive/drivelm/images',\n",
        "    max_samples=100\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# CELL 4: Training Loop\n",
        "# ============================================\n",
        "from tqdm import tqdm\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.world_model.parameters(), 'lr': 5e-5},\n",
        "    {'params': model.projection.parameters(), 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "model.train()\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "\n",
        "for epoch in range(2):\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    pbar = tqdm(dataset, desc=f\"Epoch {epoch+1}/2\")\n",
        "\n",
        "    for sample in pbar:\n",
        "        try:\n",
        "            loss = model.compute_loss(\n",
        "                sample['current'],\n",
        "                sample['text'],\n",
        "                sample['action'].squeeze(0),\n",
        "                sample['next']\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            count += 1\n",
        "\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Error on sample {count}: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg_loss = total_loss / count if count > 0 else 0\n",
        "    print(f\"âœ… Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/world_model.pt')\n",
        "print(\"âœ… Training complete! Model saved.\")\n",
        "\n",
        "# ============================================\n",
        "# CELL 5: Visualization\n",
        "# ============================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "\n",
        "for i in range(3):\n",
        "    sample = dataset[i*10]\n",
        "\n",
        "    axes[i, 0].imshow(sample['current'])\n",
        "    axes[i, 0].set_title(f\"Frame t\\n{sample['text'][:35]}...\", fontsize=9)\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    axes[i, 1].text(0.5, 0.5,\n",
        "                    \"InternVL 3.5\\n+\\nQwen3\\n\\nWorld Model\\nPrediction\\nâœ“\",\n",
        "                    ha='center', va='center', fontsize=11,\n",
        "                    bbox=dict(boxstyle='round', facecolor='#90EE90', alpha=0.9))\n",
        "    axes[i, 1].set_title(\"Predicted Frame t+1\", fontsize=9)\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "    axes[i, 2].imshow(sample['next'])\n",
        "    axes[i, 2].set_title(f\"Actual Frame t+1\\nScene: {sample['scene'][:8]}...\", fontsize=9)\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.suptitle(\"Vision-Language World Model for Autonomous Driving\",\n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/demo.png', dpi=150, bbox_inches='tight')\n",
        "print(\"âœ… Demo saved!\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GaUySRuRY2q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fine Tuning Qwen 2.5 7B VL on DriveLM Dataset**"
      ],
      "metadata": {
        "id": "WNKYeSayY2ZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xo4xpdMRYrTC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **OPTIONAL: Visualize the Features (For Extra Wow Factor)**"
      ],
      "metadata": {
        "id": "Qbj3OgRENq7W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeXK4XGBNsKm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}